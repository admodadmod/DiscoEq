{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import   pandas  as  pd\n",
    "import   numpy   as  np\n",
    "import   seaborn as  sns\n",
    "import   matplotlib.pyplot as plt ; plt.rcParams[\"font.family\"] = \"DejaVu Serif\"\n",
    "from     sklearn.metrics         import mean_squared_error, r2_score, mean_absolute_percentage_error, median_absolute_error\n",
    "from     sklearn.model_selection import cross_val_score,GridSearchCV, train_test_split, RepeatedKFold\n",
    "from     sklearn.preprocessing   import StandardScaler, MinMaxScaler\n",
    "#from deepforest import CascadeForestRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "    Calculate Mean Squared Error (MSE) between true and predicted values.\n",
    "\n",
    "    Parameters:\n",
    "        -- y_true (numpy array or list): True values\n",
    "        -- y_pred (numpy array or list): Predicted values\n",
    "\n",
    "    Returns:\n",
    "        -- mse (float): Mean Squared Error\n",
    "    \"\"\"\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "\n",
    "    #convert then into umpy array if not already\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    #squared the differences\n",
    "    squared_diff = (y_true - y_pred)**2\n",
    "    #find the avg of squared_diff\n",
    "    mse = np.mean(squared_diff)\n",
    "    return mse\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Calculate Root Mean Squared Error (RMSE) between true and predicted values.\n",
    "\n",
    "    Parameters:\n",
    "        -- y_true (numpy array or list): True values\n",
    "        -- y_pred (numpy array or list): Predicted values\n",
    "\n",
    "    Returns:\n",
    "        -- mse (float): Mean Squared Error\n",
    "    \"\"\"\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "\n",
    "    #convert then into umpy array if not already\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    #squared the differences\n",
    "    squared_diff = (y_true - y_pred)**2\n",
    "    #find the avg of squared_diff\n",
    "    mse = np.mean(squared_diff)\n",
    "    return np.sqrt(mse)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Calculate Mean Absolute Error (MAE) between true and predicted values.\n",
    "\n",
    "    Parameters:\n",
    "        -- y_true (numpy array or list): True values\n",
    "        -- y_pred (numpy array or list): Predicted values\n",
    "\n",
    "    Returns:\n",
    "        -- mae (float): Mean Absolute Error\n",
    "    \"\"\"\n",
    "\n",
    "def mean_absolute_error(y_true, y_pred):\n",
    "\n",
    "    #convert then into umpy array if not already\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    #squared the differences\n",
    "    absolute_diff = np.absolute(y_true - y_pred)\n",
    "    #find the avg of squared_diff\n",
    "    mae = np.mean(absolute_diff)\n",
    "    return mae\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Calculate R-squared value given predicted value and actual label\n",
    "\n",
    "    Parameters:\n",
    "        -- y_true (numpy array or list): True values\n",
    "        -- y_pred (numpy array or list): Predicted values\n",
    "\n",
    "    Returns:\n",
    "        -- r^2 (float): Coefficient of determination(R^2)\n",
    "    \"\"\"\n",
    "\n",
    "# def r2_score(y_true, y_pred):\n",
    "\n",
    "#     #convert then into umpy array if not already\n",
    "#     y_true = np.array(y_true)\n",
    "#     y_pred = np.array(y_pred)\n",
    "#     #sum of squared residuals\n",
    "#     SSR = np.sum((y_true - y_pred)**2)\n",
    "#     #sum of squared total\n",
    "#     y_avg = np.mean(y_true)\n",
    "#     SST = np.sum((y_avg-y_true)**2)\n",
    "#     return 1 - (float(SSR)/float(SST))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Calculate Adjusted R-squared value given predicted and actual label and number of predictors\n",
    "\n",
    "    Parameters:\n",
    "        -- y_true (numpy array or list): True values\n",
    "        -- y_pred (numpy array or list): Predicted values\n",
    "        -- k (integer): Number of predictors\n",
    "\n",
    "    Returns:\n",
    "        -- adjusted r^2 (float): Coefficient of determination(R^2)\n",
    "    \"\"\"\n",
    "\n",
    "def adjusted_r2_score(y_true, y_pred, k):\n",
    "\n",
    "    #convert then into umpy array if not already\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    #number of observations\n",
    "    n = len(y_true)\n",
    "    #sum of squared residuals\n",
    "    SSR = np.sum((y_true - y_pred)**2)\n",
    "    #sum of squared total\n",
    "    y_avg = np.mean(y_true)\n",
    "    SST = np.sum((y_avg-y_true)**2)\n",
    "\n",
    "    #r2_score\n",
    "    r2_score = 1 - (float(SSR)/float(SST))\n",
    "    #adjusted r2 square\n",
    "    r2_score_adj = 1 - float((1-r2_score**2)*(n-1))/float(n - k - 1)\n",
    "\n",
    "    return r2_score_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  python== 3.9.13\n",
    "### pandas==1.3.3 \\    numpy==1.23.5  \\ openpyxl==3.0.10  \\  matplotlib==3.5.1\n",
    "### xgboost==1.7.3 \\ shap==0.45.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PreProc**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = pd.read_excel('./Data.xlsx')\n",
    "data2 = data3.drop(columns=['CEC','HM','T', 'SA',\t'Label','η','Ncharge'], axis=1)\n",
    "print(data2.shape); data2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = {'pH_solution':'PHS', '(O+N)/C':'ONC','pH_biochar':'PHB','(H-O-2N)/C':'HO2NC'}\n",
    "data2 = data2.rename(columns=new_columns)\n",
    "print(data2.shape); data2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find duplicate rows\n",
    "duplicates = data2[data2.duplicated()]\n",
    "## Display and drop the duplicate rows\n",
    "print(\"number of duplicates shape:\",duplicates.shape)\n",
    "data_no_duplicates = data2.drop_duplicates()\n",
    "print(\"data_no_duplicates shape:\", data_no_duplicates.shape)\n",
    "data_no_duplicates.head(2)\n",
    "\n",
    "# data_no_duplicates = data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the category names corresponding to the numeric codes\n",
    "# category_mapping = dict(enumerate(data_no_duplicates['HM'].astype('category').cat.categories))\n",
    "\n",
    "# # View the mapping\n",
    "# print(category_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# ## Apply OneHotEncoder\n",
    "# encoder = OneHotEncoder(sparse_output=False)\n",
    "# encoded_data = encoder.fit_transform(data_no_duplicates[['HM']])\n",
    "\n",
    "# ## Convert the result back to a DataFrame\n",
    "# encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(['HM']))\n",
    "# print(encoded_data[:2]); print(\"encoded_df shape is:\", encoded_df.shape)\n",
    "\n",
    "# ## Drop the original 'Metal_type' column\n",
    "# data = data_no_duplicates.drop('HM', axis=1)\n",
    "\n",
    "# ## Concatenate the encoded columns with the original DataFrame\n",
    "# data.reset_index(drop=True, inplace=True)\n",
    "# encoded_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# data = pd.concat([data, encoded_df], axis=1)\n",
    "\n",
    "\n",
    "# ## Ensure 'RE' is the last column\n",
    "# columns = [col for col in data.columns if col != 'RE'] + ['RE']\n",
    "# data = data[columns]\n",
    "\n",
    "# ## Now 'data' contains the one-hot encoded columns, and 'AE' is the last column\n",
    "# data = data.drop(columns=['O/C','ONC'], axis=1)\n",
    "# print(\"\\n\", data.head(2)); print(\"data shape is:\", data.shape)\n",
    "\n",
    "# Get rows with NaN\n",
    "# rows_with_nan = data[data.isna().any(axis=1)]\n",
    "# print(rows_with_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rows with NaN\n",
    "rows_with_nan = data_no_duplicates[data_no_duplicates.isna().any(axis=1)]\n",
    "print(rows_with_nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get category names corresponding to the numeric codes\n",
    "# categories = encoder.categories_[0]  # Since we're encoding just one column ('Metal_type')\n",
    "\n",
    "# # Print the category names\n",
    "# print(\"Category names:\", categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = data_no_duplicates.iloc[:,:-1]\n",
    "output = data_no_duplicates.iloc[:,-1]\n",
    "print(inputs.shape)\n",
    "inputs.head()\n",
    "\n",
    "# Cd2+ = [0.0000\t1.0000\t0.0000\t0.0000\t0.0000\t0.0000]\n",
    "# Cu2+ = [0.0000\t0.0000\t1.0000\t0.0000\t0.0000\t0.0000]\n",
    "# Pb2+ = [0.0000\t0.0000\t0.0000\t0.0000\t1.0000\t0.0000]\n",
    "# Zn2+ = [0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t1.0000]\n",
    "# As3+ = [1.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000]\n",
    "# Ni2+ = [0.0000\t0.0000\t0.0000\t1.0000\t0.0000\t0.0000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.4f}'.format\n",
    "inputs.iloc[:,:-6].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs.dtypes)\n",
    "# print(inputs['C0'].unique())\n",
    "# Get the unique values in the 'C0' column\n",
    "unique_values = inputs['C0'].unique()\n",
    "\n",
    "# Loop through the unique values and find their indices\n",
    "for value in unique_values:\n",
    "    indices = inputs[inputs['C0'] == value].index\n",
    "    # print(f\"Value: {value} - Indices: {indices.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### corr = data_mod_lc.drop('RE (%)', axis=1).corr() # examining correlations\n",
    "\n",
    "### Plotting correlation above or below 0.5\n",
    "### df = data.drop(columns=['Zinc', 'Cadmium', 'Arsenic'], axis=1)\n",
    "\n",
    "### new_columns = {'Siq (MPa)':'CSR', 'Hs (m)':'PLS','Hr (m)':'PLR','B (m)':'PD' }\n",
    "### df = data.rename(columns=new_columns)\n",
    "\n",
    "# corr = data.iloc[:,:-1].corr()\n",
    "\n",
    "# plt.rcParams[\"font.family\"] = \"DejaVu Serif\"\n",
    "\n",
    "# plt.figure(figsize=(12, 10))\n",
    "\n",
    "# sns.heatmap(corr[(corr >= 0.0) | (corr <= -0.0)],\n",
    "#             cmap='viridis', vmax=1.0, vmin=-1.0, linewidths=0.1,\n",
    "#             annot=True, annot_kws={\"size\": 8}, square=True);\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('./Res/Figs/corrEL_2.pdf', dpi=600, bbox_inches='tight', format = 'pdf')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split and Save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "import pandas as pd\n",
    "\n",
    "## Assuming 'data_no_duplicates' is DataFrame and you want to remove rows with outliers\n",
    "\n",
    "## Define the IsolationForest model\n",
    "iso_forest = IsolationForest(random_state=42)  # Adjust contamination based on expected outlier percentage\n",
    "\n",
    "## Fit the model and predict outliers\n",
    "aaaaa = pd.DataFrame(data_no_duplicates)\n",
    "aaaaa.to_excel('./TTV/data_with_outliers.xlsx', index= False)\n",
    "isodata = iso_forest.fit_predict(data_no_duplicates)\n",
    "\n",
    "## Filter out outliers (outliers will have a label of -1)\n",
    "data_no_outliers = data_no_duplicates[isodata != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = data_no_outliers.iloc[:,:-1]\n",
    "output = data_no_outliers.iloc[:,-1]\n",
    "print(inputs.shape)\n",
    "inputs.head(3)\n",
    "\n",
    "# aaaaa = pd.DataFrame(data_no_outliers)\n",
    "# aaaaa.to_excel('./TTV/withoutOutlier/data_no_outliers.xlsx', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train_Test_Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 1. Separate your data (as you already have)\n",
    "# Note: Ensure 'Label' is dropped first if it's still in the inputs, \n",
    "# as scalers only work on numbers.\n",
    "\n",
    "# inputs = data_no_outliers.iloc[:, :-1] # Assuming Label is the 2nd to last column\n",
    "# output = data_no_outliers.iloc[:, -1]\n",
    "inputs = data_no_duplicates.iloc[:,:-1]\n",
    "output = data_no_duplicates.iloc[:,-1]\n",
    "\n",
    "\n",
    "# 2. Initialize the scalers\n",
    "scaler_in = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_out = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# 3. Scale the Inputs\n",
    "# This returns a numpy array. We convert back to DataFrame to keep column names.\n",
    "inputs_scaled = pd.DataFrame(\n",
    "    scaler_in.fit_transform(inputs), \n",
    "    columns=inputs.columns\n",
    ")\n",
    "\n",
    "# 4. Scale the Output\n",
    "# Scikit-learn expects a 2D array for the output, so we use .values.reshape(-1, 1)\n",
    "output_scaled = scaler_out.fit_transform(output.values.reshape(-1, 1))\n",
    "\n",
    "# Flatten it back to a 1D array/series for the regression model\n",
    "output_scaled = output_scaled.flatten()\n",
    "\n",
    "# --- Verification ---\n",
    "print(f\"Input Max: {inputs_scaled.max().max()}\") # Should be 1.0\n",
    "print(f\"Output Max: {output_scaled.max()}\")      # Should be 1.0\n",
    "\n",
    "\n",
    "trainx1, testx1, trainy1, testy1= train_test_split(inputs_scaled, output_scaled, test_size=0.2, random_state=22 , shuffle=True)\n",
    "\n",
    "# trainx1, testx1, trainy1, testy1= train_test_split(inputs, output, test_size=0.2, random_state=22 , shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aaaaa = pd.DataFrame(trainx1)\n",
    "# aaaaa.to_excel('./TTV/withoutOutlier/trainx.xlsx', index= False)\n",
    "# bbbb = pd.DataFrame(trainy1)\n",
    "# bbbb.to_excel('./TTV/withoutOutlier/trainy.xlsx', index= False)\n",
    "\n",
    "# aaaaa = pd.DataFrame(testx1)\n",
    "# aaaaa.to_excel('./TTV/withoutOutlier/testx.xlsx', index= False)\n",
    "# bbbb = pd.DataFrame(testy1)\n",
    "# bbbb.to_excel('./TTV/withoutOutlier/testy.xlsx')\n",
    "\n",
    "trainx2 = pd.read_excel('./TTV/withoutOutlier/trainx.xlsx')\n",
    "trainy2 = pd.read_excel('./TTV/withoutOutlier/trainy.xlsx')\n",
    "testx2 = pd.read_excel('./TTV/withoutOutlier/testx.xlsx')\n",
    "testy2 = pd.read_excel('./TTV/withoutOutlier/testy.xlsx')\n",
    "\n",
    "\n",
    "# testy2 = testy2.iloc[:,1]\n",
    "# trainy2 = trainy2.iloc[:,1]\n",
    "\n",
    "# trainx2 = trainx2.iloc[:,1:]\n",
    "# testx2 = testx2.iloc[:,1:]\n",
    "\n",
    "# x_train = trainx2.to_numpy()\n",
    "# x_test =  testx2.values.astype(np.float64)\n",
    "\n",
    "# y_test  = testy2\n",
    "# y_train = trainy2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aaaaa = pd.DataFrame(trainx1)\n",
    "# aaaaa.to_excel('./TTV/trainx.xlsx', index= False)\n",
    "# bbbb = pd.DataFrame(trainy1)\n",
    "# bbbb.to_excel('./TTV/trainy.xlsx', index= False)\n",
    "\n",
    "# aaaaa = pd.DataFrame(testx1)\n",
    "# aaaaa.to_excel('./TTV/testx.xlsx', index= False)\n",
    "# bbbb = pd.DataFrame(testy1)\n",
    "# bbbb.to_excel('./TTV/testy.xlsx')\n",
    "\n",
    "trainx2 = pd.read_excel('./TTV/trainx.xlsx')\n",
    "trainy2 = pd.read_excel('./TTV/trainy.xlsx')\n",
    "testx2 = pd.read_excel('./TTV/testx.xlsx')\n",
    "testy2 = pd.read_excel('./TTV/testy.xlsx')\n",
    "\n",
    "\n",
    "# testy2 = testy2.iloc[:,1]\n",
    "# trainy2 = trainy2.iloc[:,1]\n",
    "\n",
    "# trainx2 = trainx2.iloc[:,1:]\n",
    "# testx2 = testx2.iloc[:,1:]\n",
    "\n",
    "# x_train = trainx2.to_numpy()\n",
    "# x_test =  testx2.values.astype(np.float64)\n",
    "\n",
    "# y_test  = testy2\n",
    "# y_train = trainy2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAnetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a generative adversarial network on a one-dimensional function\n",
    "from numpy import hstack\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import rand\n",
    "from numpy.random import randn\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# define the standalone discriminator model\n",
    "def define_discriminator(n_inputs=2):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(n_inputs,)))\n",
    "    model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(60, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(20, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# define the standalone generator model\n",
    "def define_generator(latent_dim, n_outputs=2):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(latent_dim,)))\n",
    "    model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(60, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(20, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(n_outputs, activation='linear'))\n",
    "    return model\n",
    "\n",
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(generator, discriminator):\n",
    "    # make weights in the discriminator not trainable\n",
    "    discriminator.trainable = False\n",
    "    # connect them\n",
    "    model = Sequential()\n",
    "    # add generator\n",
    "    model.add(generator)\n",
    "    # add the discriminator\n",
    "    model.add(discriminator)\n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# generate n real samples with class labels\n",
    "def generate_real_samples(n):\n",
    "    # # generate inputs in [-0.5, 0.5]\n",
    "    # X1 = rand(n) - 0.5\n",
    "    # # generate outputs X^2\n",
    "    # X2 = X1 * X1\n",
    "    # # stack arrays\n",
    "    # X1 = X1.reshape(n, 1)\n",
    "    # X2 = X2.reshape(n, 1)\n",
    "    # X = hstack((X1, X2))\n",
    "    # # generate class labels\n",
    "    # y = ones((n, 1))\n",
    "    X = XN\n",
    "    y = YN\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n):\n",
    "    # generate points in the latent space\n",
    "    x_input = randn(latent_dim * n)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    x_input = x_input.reshape(n, latent_dim)\n",
    "    return x_input\n",
    "\n",
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(generator, latent_dim, n):\n",
    "    # generate points in latent space\n",
    "    x_input = generate_latent_points(latent_dim, n)\n",
    "    # predict outputs\n",
    "    X = generator.predict(x_input)\n",
    "    # create class labels\n",
    "    y = zeros((n, 1))\n",
    "    return X, y\n",
    "\n",
    "# evaluate the discriminator and plot real and fake points\n",
    "def summarize_performance(epoch, generator, discriminator, latent_dim, n=100):\n",
    "    # prepare real samples\n",
    "    x_real, y_real = generate_real_samples(n)\n",
    "    # evaluate discriminator on real examples\n",
    "    _, acc_real = discriminator.evaluate(x_real, y_real, verbose=0)\n",
    "    # prepare fake examples\n",
    "    x_fake, y_fake = generate_fake_samples(generator, latent_dim, n)\n",
    "    # evaluate discriminator on fake examples\n",
    "    _, acc_fake = discriminator.evaluate(x_fake, y_fake, verbose=0)\n",
    "    # summarize discriminator performance\n",
    "    print(epoch, acc_real, acc_fake)\n",
    "    # scatter plot real and fake data points\n",
    "    pyplot.scatter(x_real[:, 0], x_real[:, 1], color='red')\n",
    "    pyplot.scatter(x_fake[:, 0], x_fake[:, 1], color='blue')\n",
    "    pyplot.show()\n",
    "\n",
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, gan_model, latent_dim, n_epochs=10000, n_batch=128, n_eval=2000):\n",
    "    # determine half the size of one batch, for updating the discriminator\n",
    "    half_batch = int(n_batch / 2)\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_epochs):\n",
    "        # prepare real samples\n",
    "        x_real, y_real = generate_real_samples(half_batch) \n",
    "        # prepare fake examples\n",
    "        x_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch) \n",
    "        # update discriminator\n",
    "        d_model.train_on_batch(x_real, y_real, verbose=0) \n",
    "        d_model.train_on_batch(x_fake, y_fake, verbose=0) \n",
    "        # prepare points in latent space as input for the generator\n",
    "        x_gan = generate_latent_points(latent_dim, n_batch) \n",
    "        # create inverted labels for the fake samples\n",
    "        y_gan = ones((n_batch, 1))\n",
    "        # update the generator via the discriminator's error\n",
    "        gan_model.train_on_batch(x_gan, y_gan, verbose=0) \n",
    "        # evaluate the model every n_eval epochs\n",
    "        # if (i+1) % n_eval == 0:\n",
    "            #summarize_performance(i, g_model, d_model, latent_dim)\n",
    "\n",
    "# size of the latent space\n",
    "latent_dim = 19\n",
    "# create the discriminator\n",
    "discriminator = define_discriminator()\n",
    "# create the generator\n",
    "generator = define_generator(latent_dim)\n",
    "# create the gan\n",
    "gan_model = define_gan(generator, discriminator)\n",
    "# train model\n",
    "train(generator, discriminator, gan_model, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C1 without outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx2 = pd.read_excel('./TTV/withoutOutlier/trainx.xlsx')\n",
    "trainy2 = pd.read_excel('./TTV/withoutOutlier/trainy.xlsx')\n",
    "testx2 = pd.read_excel('./TTV/withoutOutlier/testx.xlsx')\n",
    "testy2 = pd.read_excel('./TTV/withoutOutlier/testy.xlsx')\n",
    "trainy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx = pd.read_excel('./TTV/withoutOutlier/trainx.xlsx')\n",
    "trainy = pd.read_excel('./TTV/withoutOutlier/trainy.xlsx').values.ravel()\n",
    "testx = pd.read_excel('./TTV/withoutOutlier/testx.xlsx')\n",
    "testy = pd.read_excel('./TTV/withoutOutlier/testy.xlsx').values.ravel()\n",
    "trainx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'KAN' from 'kan' (/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/kan/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkan\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KAN\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m r2_score\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler, MinMaxScaler\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'KAN' from 'kan' (/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/kan/__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from kan import KAN\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. PATHS & LOADING\n",
    "# ---------------------------------------------------------\n",
    "save_path = r'./Res/kan'\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "def load_clean(path):\n",
    "    df = pd.read_excel(path).dropna(how='all')\n",
    "    return df.apply(pd.to_numeric, errors='coerce').dropna()\n",
    "\n",
    "trainx_df = load_clean('./TTV/trainx.xlsx')\n",
    "trainy_df = load_clean('./trainy.xlsx')\n",
    "testx_df  = load_clean('./testx.xlsx')\n",
    "testy_df  = load_clean('./TTV/testy.xlsx')\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. HYBRID PREPROCESSING\n",
    "# ---------------------------------------------------------\n",
    "# StandardScaler for inputs (handles different units better)\n",
    "scaler_x = StandardScaler()\n",
    "# MinMaxScaler for output (keeps target in 0-1 range)\n",
    "scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "X_train_s = scaler_x.fit_transform(trainx_df.values)\n",
    "y_train_s = scaler_y.fit_transform(trainy_df.values.reshape(-1, 1))\n",
    "X_test_s  = scaler_x.transform(testx_df.values)\n",
    "y_test_s  = scaler_y.transform(testy_df.values.reshape(-1, 1))\n",
    "\n",
    "dataset = {\n",
    "    'train_input': torch.tensor(X_train_s, dtype=torch.float32),\n",
    "    'train_label': torch.tensor(y_train_s, dtype=torch.float32),\n",
    "    'test_input':  torch.tensor(X_test_s,  dtype=torch.float32),\n",
    "    'test_label':  torch.tensor(y_test_s,  dtype=torch.float32)\n",
    "}\n",
    "\n",
    "d = X_train_s.shape[1]\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. MODEL DEFINITION (Single Hidden Layer - High Stability)\n",
    "# ---------------------------------------------------------\n",
    "print(f\"\\n>>> Initializing KAN [{d}, 8, 1]...\")\n",
    "# A single wide layer (8) is often better than two thin layers\n",
    "model = KAN(width=[d, 8, 1], grid=3, k=3, seed=42)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. BALANCED TRAINING STRATEGY\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Stage 1: Warmup with very coarse grid\n",
    "print(\">>> Stage 1: Adam Warmup (Grid 3)\")\n",
    "model.fit(dataset, opt=\"Adam\", steps=300, lamb=0.002)\n",
    "\n",
    "# Stage 2: Increase resolution\n",
    "print(\">>> Stage 2: Grid Extension (Grid 10)\")\n",
    "model = model.refine(10)\n",
    "model.fit(dataset, opt=\"Adam\", steps=300, lamb=0.002)\n",
    "\n",
    "# Stage 3: Fine Tuning with LBFGS\n",
    "print(\">>> Stage 3: LBFGS Precision Tuning (Grid 20)\")\n",
    "model = model.refine(20)\n",
    "model.fit(dataset, opt=\"LBFGS\", steps=100, lamb=0.001, update_grid=False)\n",
    "\n",
    "# Stage 4: Pruning\n",
    "print(\">>> Stage 4: Pruning\")\n",
    "model = model.prune()\n",
    "# Final polish after pruning\n",
    "model.fit(dataset, opt=\"LBFGS\", steps=50, update_grid=False)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. EVALUATION\n",
    "# ---------------------------------------------------------\n",
    "y_train_pred_s = model(dataset['train_input']).detach().numpy()\n",
    "y_test_pred_s  = model(dataset['test_input']).detach().numpy()\n",
    "\n",
    "y_train_pred = scaler_y.inverse_transform(y_train_pred_s).ravel()\n",
    "y_test_pred  = scaler_y.inverse_transform(y_test_pred_s).ravel()\n",
    "y_train_real = scaler_y.inverse_transform(y_train_s).ravel()\n",
    "y_test_real  = scaler_y.inverse_transform(y_test_s).ravel()\n",
    "\n",
    "# Clip predictions to 0-1 range to handle spline overshoots\n",
    "y_train_pred = np.clip(y_train_pred, 0, 1)\n",
    "y_test_pred = np.clip(y_test_pred, 0, 1)\n",
    "\n",
    "r2_train = r2_score(y_train_real, y_train_pred)\n",
    "r2_test  = r2_score(y_test_real,  y_test_pred)\n",
    "\n",
    "print(f\"\\n================ FINAL RESULTS ================\")\n",
    "print(f\"Train R² : {r2_train:.4f}\")\n",
    "print(f\"Test  R² : {r2_test:.4f}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6. PLOTTING\n",
    "# ---------------------------------------------------------\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.scatter(y_train_real, y_train_pred, alpha=0.3, label=f'Train (R²={r2_train:.2f})', color='royalblue')\n",
    "plt.scatter(y_test_real, y_test_pred, alpha=0.7, edgecolors='k', label=f'Test (R²={r2_test:.2f})', color='darkorange')\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Ideal')\n",
    "plt.xlabel('Measured')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title(f'Balanced PyKAN (Test R²: {r2_test:.3f})')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7. Symbolic Formula\n",
    "try:\n",
    "    model.auto_symbolic(lib=['x', 'x^2', 'exp', 'sin'], verbose=False)\n",
    "    formula = model.symbolic_formula()[0][0]\n",
    "    print(\"\\nExtracted Equation:\\n\", formula)\n",
    "except:\n",
    "    print(\"\\nVisualizing via model.plot()...\")\n",
    "    model.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pysr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pysindy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gplearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn\n",
    "# print(\"scikit-learn version:\", sklearn.__version__)\n",
    "# scikit-learn version: 1.7.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
